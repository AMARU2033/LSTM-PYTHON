{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from theano.gradient import grad_clip\n",
    "from utils import *\n",
    "from lstm_theano import *\n",
    "from gru_theano import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Stanford pre-trained glove vectors\n",
    "gdic, gvec = load_stanford_glove(\"data/glove/glove.6B.100d.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 70693 sentences.\n",
      "Found 57636 unique words tokens.\n",
      "Using vocabulary size 2000.\n",
      "The least frequent word in our vocabulary is 'offensive' and appeared 54 times.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from utils import load_and_proprocess_data\n",
    "VOCABULARY_SIZE = 2000\n",
    "X_train, y_train, word_to_index, index_to_word = load_and_proprocess_data(VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct glove word vectors for vocabulary\n",
    "wv_dim = gvec.shape[1]\n",
    "wv = []\n",
    "for i in range(VOCABULARY_SIZE):\n",
    "    word = index_to_word[i]\n",
    "    if word not in gdic:\n",
    "        wv.append(np.zeros(wv_dim))\n",
    "    else:\n",
    "        wv.append(gvec[gdic[word]])\n",
    "wv = np.array(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a gradient check\n",
    "np.random.seed(0)\n",
    "model = GRUTheano(100, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4, reg_lambda=0.001, wordvec=None):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        # Initialize the network parameters\n",
    "        if wordvec != None:\n",
    "            U = np.array([wordvec.T, wordvec.T, wordvec.T])\n",
    "        else:\n",
    "            U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (3, hidden_dim, word_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (3, hidden_dim, hidden_dim))\n",
    "        b = np.zeros((3, hidden_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        b2 = np.zeros(word_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        # Bias terms\n",
    "        self.b = theano.shared(name='b_i', value=b.astype(theano.config.floatX))\n",
    "        self.b2 = theano.shared(name='b_V', value=b2.astype(theano.config.floatX))\n",
    "        # SGD: Initialize parameters\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        self.mb = theano.shared(name='mb', value=np.zeros(b.shape).astype(theano.config.floatX))\n",
    "        self.mb2 = theano.shared(name='mb2', value=np.zeros(b2.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        V, U, W, b, b2 = self.V, self.U, self.W, self.b, self.b2\n",
    "        # mV, mU, mW, mb, mb2 = self.mV, self.mU, self.mW, self.mb, self.mb2\n",
    "        \n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t_prev):\n",
    "            # This is how we calculated the hidden state in a simple RNN. No longer!\n",
    "            # s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            \n",
    "            # Clip the gradients\n",
    "            W_clipped = grad_clip(W, -1, 1)\n",
    "            U_clipped = grad_clip(U, -1, 1)\n",
    "            V_clipped = grad_clip(V, -1, 1)\n",
    "            b_clipped = grad_clip(b, -1, 1)\n",
    "            b2_clipped = grad_clip(b2, -1, 1)\n",
    "            \n",
    "            # LRU hidden state calculation\n",
    "            z_t = T.nnet.sigmoid(U_clipped[0][:,x_t] + W_clipped[0].dot(s_t_prev) + b_clipped[0])\n",
    "            r_t = T.nnet.sigmoid(U_clipped[1][:,x_t] + W_clipped[1].dot(s_t_prev) + b_clipped[1])\n",
    "            c_t = T.tanh(U_clipped[2][:,x_t] + W_clipped[2].dot(s_t_prev) * r_t + b_clipped[2])\n",
    "            s_t = (1 - z_t) * c_t + z_t * s_t_prev\n",
    "              \n",
    "            # Final output calculation\n",
    "            # Theano's softmax returns a matrix with one row, we only need the row\n",
    "            o_t = T.nnet.softmax(V_clipped.dot(s_t) + b2_clipped)[0]\n",
    "\n",
    "            return [o_t, s_t]\n",
    "        \n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            truncate_gradient=self.bptt_truncate)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Regularization cost\n",
    "        reg_cost = self.reg_lambda/2. * \\\n",
    "            (T.sum(T.sqr(V)) + T.sum(T.sqr(U)) + T.sum(T.sqr(W)) + T.sum(T.sqr(b)) + T.sum(T.sqr(b2)))\n",
    "        # Total cost\n",
    "        cost = o_error + reg_cost\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dW = T.grad(o_error, W)\n",
    "        db = T.grad(o_error, b)\n",
    "        dV = T.grad(o_error, V)\n",
    "        db2 = T.grad(o_error, b2)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], cost)\n",
    "        self.bptt = theano.function([x, y], [dU, dW, db, dV, db2])\n",
    "        \n",
    "        # SGD parameters\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        \n",
    "        # rmsprop cache updates\n",
    "        mU = decay * self.mU + (1 - decay) * T.sqr(dU)\n",
    "        mW = decay * self.mW + (1 - decay) * T.sqr(dW)\n",
    "        mV = decay * self.mV + (1 - decay) * T.sqr(dV)\n",
    "        mb = decay * self.mb + (1 - decay) * T.sqr(db)\n",
    "        mb2 = decay * self.mb2 + (1 - decay) * T.sqr(db2)\n",
    "        \n",
    "        self.sgd_step = theano.function(\n",
    "            [x, y, learning_rate, theano.Param(decay, default=0.9)],\n",
    "            [], \n",
    "            updates=[(U, U - learning_rate * dU / T.sqrt(mU + 1e-8)),                     \n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-8)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-8)),\n",
    "                     (b, b - learning_rate * db / T.sqrt(mb + 1e-8)),\n",
    "                     (b2, b2 - learning_rate * db2 / T.sqrt(mb2 + 1e-8)),\n",
    "                     (self.mU, mU),\n",
    "                     (self.mW, mW),\n",
    "                     (self.mV, mV),\n",
    "                     (self.mb, mb),\n",
    "                     (self.mb2, mb2)\n",
    "                    ])\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: 120.093107 milliseconds\n",
      "2015-10-19 09:47:04: Loss after num_examples_seen=0 epoch=0: 7.642890\n",
      "2015-10-19 09:47:47: Loss after num_examples_seen=500 epoch=1: 6.229052\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of pre-trained model\n",
    "# model = load_model_parameters_theano('./data/2015-10-18/GRUTheano-80-8000-10.npz', GRUTheano)\n",
    "\n",
    "# Build model and train\n",
    "\n",
    "REGULARIZATION = 0.00\n",
    "LEARNING_RATE = 1e-4\n",
    "NEPOCH = 50\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "model = GRUTheano(VOCABULARY_SIZE, hidden_dim=HIDDEN_DIM, reg_lambda=REGULARIZATION, wordvec=wv)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[10], y_train[10], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train[:500], y_train[:500], LEARNING_RATE, NEPOCH, evaluate_loss_after=1, decay=0.9)\n",
    "# save_model_parameters_theano('data/mymodel.npz', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 negative choice through it to are the '' .\n",
      "0.000000 costs threads your loss , ?\n",
      "0.000000 fill from you future do .\n",
      "0.000000 location thought pushing jail to it even .\n",
      "0.000000 update system fear partner core stats issues .\n",
      "0.000000 since 40 , vs you .\n",
      "0.000000 anyways reasonable ten ? citizens .\n",
      "0.000000 proper not $ bad the this .\n",
      "0.000000 twice obviously as top it ,\n",
      "0.000000 various dude racism certain with .\n",
      "0.000000 bot pc address '' parents it .\n",
      "0.000000 3 note ships already '' .\n",
      "0.000000 may companies '' none it .\n",
      "0.000000 youtube expect into you it !\n",
      "0.000000 memory benefits him for ) .\n",
      "0.000000 contact civil spend damn is or number .\n",
      "0.000000 again shitty sort decent compared the performance books to it\n",
      "0.000000 hot proof yourself of the everybody deck .\n",
      "0.000000 separate boring , stupid yourself .\n",
      "0.000000 shitty here `` them first .\n",
      "0.000000 moving range personal split fighting to ? you\n",
      "0.000000 slow - major well it wrote\n",
      "0.000000 suppose followed case it sound for ''\n",
      "0.000000 sad before changed intended surface definitely .\n",
      "0.000000 customers sad potential five armor religious .\n",
      "0.000000 account did do is top .\n",
      "0.000000 profile check there for needed is for eye the !\n",
      "0.000000 hitting speaking on n't it moving .\n",
      "0.000000 profit weak almost the ? ?\n",
      "0.000000 gon google air likes provide awesome .\n",
      "0.000000 murder stated soon for bible .\n",
      "0.000000 treat necessarily since you the .\n",
      "0.000000 depending dragon months of arm ''\n",
      "0.000000 console 4 think limited lived .\n",
      "0.000000 film looks camera anything do it to\n",
      "0.000000 mechanics of same the somewhere .\n",
      "0.000000 brain slightly smoking show tools .\n",
      "0.000000 hours unless settings ! way less them point to\n",
      "0.000000 will hp cover wait for common .\n",
      "0.000000 planned hunter hundreds looking effect blood , and .\n",
      "0.000000 leading network ring better at color definition .\n",
      "0.000000 wow friends time short remove .\n",
      "0.000000 seriously full three practice '' .\n",
      "0.000000 idiot dropped universe bible like .\n",
      "0.000000 fairly gender prefer funny boring .\n",
      "0.000000 oil now scenario it our ?\n",
      "0.000000 blame period otherwise album or ?\n",
      "0.000000 fuck mistake actually cops for topic boring .\n",
      "0.000000 per economy '' and contact listed .\n",
      "0.000000 shoes working do win to ! .\n"
     ]
    }
   ],
   "source": [
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    \n",
    "# TODO: Beam prediction\n",
    "def generate_sentence(model, beam_size=100, max_length=6):\n",
    "    # We start the sentence with the start token and a random word\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    prob = 1\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "        sampled_word = np.argmax(samples)\n",
    "        prob = prob * next_word_probs[-1][sampled_word]\n",
    "        # Discard this sentence if we sample an unknown word\n",
    "        if sampled_word == word_to_index[unknown_token]:\n",
    "           return [1,[]]\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return [prob, sentence_str]\n",
    " \n",
    "\n",
    "num_sentences = 50\n",
    "senten_min_length = 6\n",
    "sentences = []\n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        prob, sent = generate_sentence(model)\n",
    "    print \"%f %s\" % (prob, \" \".join(sent))\n",
    "    sentences.append([prob, sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
