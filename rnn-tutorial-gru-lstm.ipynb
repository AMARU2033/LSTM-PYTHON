{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from theano.gradient import grad_clip\n",
    "from utils import *\n",
    "from lstm_theano import *\n",
    "from gru_theano import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 502183 sentences.\n",
      "Found 193212 unique words tokens.\n",
      "Using vocabulary size 2000.\n",
      "The least frequent word in our vocabulary is 'tonight' and appeared 324 times.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "VOCABULARY_SIZE = 2000\n",
    "X_train, y_train, word_to_index, index_to_word = load_data(\"data/reddit-comments-2015.csv\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter E with size 1000.\n",
      "Gradient check for parameter E passed.\n",
      "Performing gradient check for parameter U with size 600.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter W with size 600.\n",
      "Gradient check for parameter W passed.\n",
      "Performing gradient check for parameter b with size 60.\n",
      "Gradient check for parameter b passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter c with size 100.\n",
      "Gradient check for parameter c passed.\n"
     ]
    }
   ],
   "source": [
    "# Do a gradient check\n",
    "np.random.seed(0)\n",
    "model = GRUTheano(100, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model model from GRU-2015-10-25-10-37-2000-48-128.npz with hidden_dim=128 word_dim=2000\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of pre-trained model\n",
    "# model = load_model_parameters_theano(\"GRU-2015-10-25-10-37-2000-48-128.npz\")\n",
    "# model = load_model_parameters_theano('./data/pretrained.npz', GRUTheano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.3986080477\n"
     ]
    }
   ],
   "source": [
    "print model.calculate_loss(X_train[:500], y_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=128, bptt_truncate=-1):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Initialize the network parameters\n",
    "        E = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        U = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (6, hidden_dim, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (6, hidden_dim, hidden_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        b = np.zeros((6, hidden_dim))\n",
    "        c = np.zeros(word_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.E = theano.shared(name='E', value=E.astype(theano.config.floatX))\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.b = theano.shared(name='b', value=b.astype(theano.config.floatX))\n",
    "        self.c = theano.shared(name='c', value=c.astype(theano.config.floatX))\n",
    "        # SGD / rmsprop: Initialize parameters\n",
    "        self.mE = theano.shared(name='mE', value=np.zeros(E.shape).astype(theano.config.floatX))\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        self.mb = theano.shared(name='mb', value=np.zeros(b.shape).astype(theano.config.floatX))\n",
    "        self.mc = theano.shared(name='mc', value=np.zeros(c.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        E, V, U, W, b, c = self.E, self.V, self.U, self.W, self.b, self.c\n",
    "        \n",
    "        U_stacked1 = T.concatenate([U[0], U[1], U[2]])\n",
    "        b_stacked1 = T.concatenate([b[0], b[1], b[2]])\n",
    "        W_stacked1 = T.concatenate([W[0], W[1], W[2]])\n",
    "        U_stacked2 = T.concatenate([U[3], U[4], U[5]])\n",
    "        b_stacked2 = T.concatenate([b[3], b[4], b[5]])\n",
    "        W_stacked2 = T.concatenate([W[3], W[4], W[5]])        \n",
    "        \n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n",
    "            # This is how we calculated the hidden state in a simple RNN. No longer!\n",
    "            # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n",
    "            \n",
    "            n = self.hidden_dim\n",
    "            # Word embedding layer\n",
    "            x_e = E[:,x_t]\n",
    "                \n",
    "            # GRU Layer 1\n",
    "            # z_t1 = T.nnet.sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\n",
    "            # r_t1 = T.nnet.sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\n",
    "            # c_t1 = T.tanh(U[2].dot(x_e) + W[2].dot(s_t1_prev * r_t1) + b[2])\n",
    "            # s_t1 = (T.ones_like(z_t1) - z_t1) * s_t1_prev + z_t1 * c_t1\n",
    "            \n",
    "            Ux1 = U_stacked1.dot(x_e) + b_stacked1\n",
    "            Ws1 = W_stacked1.dot(s_t1_prev)\n",
    "            z_t1 = T.nnet.sigmoid(Ux1[0:n] + Ws1[0:n])\n",
    "            r_t1 = T.nnet.sigmoid(Ux1[n:2*n] + Ws1[n:2*n])\n",
    "            c_t1 = T.tanh(Ux1[2*n:3*n] + Ws1[2*n:3*n] * r_t1)\n",
    "            s_t1 = (T.ones_like(z_t1) - z_t1) * s_t1_prev + z_t1 * c_t1            \n",
    "            \n",
    "            # GRU Layer 2\n",
    "            # z_t2 = T.nnet.sigmoid(U[3].dot(s_t1) + W[3].dot(s_t2_prev) + b[3])\n",
    "            # r_t2 = T.nnet.sigmoid(U[4].dot(s_t1) + W[4].dot(s_t2_prev) + b[4])\n",
    "            # c_t2 = T.tanh(U[5].dot(s_t1) + W[5].dot(s_t2_prev * r_t2) + b[5])\n",
    "            # s_t2 = (T.ones_like(z_t2) - z_t2) * s_t2_prev + z_t2 * c_t2\n",
    "\n",
    "            Ux2 = U_stacked2.dot(s_t1) + b_stacked2\n",
    "            Ws2 = W_stacked2.dot(s_t2_prev)\n",
    "            z_t2 = T.nnet.sigmoid(Ux2[0:n] + Ws2[0:n])\n",
    "            r_t2 = T.nnet.sigmoid(Ux2[n:2*n] + Ws2[n:2*n])\n",
    "            c_t2 = T.tanh(Ux2[2*n:3*n] + Ws2[2*n:3*n] * r_t2)\n",
    "            s_t2 = (T.ones_like(z_t2) - z_t2) * s_t2_prev + z_t2 * c_t2\n",
    "            \n",
    "            # Final output calculation\n",
    "            # Theano's softmax returns a matrix with one row, we only need the row\n",
    "            o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n",
    "\n",
    "            return [o_t, s_t1, s_t2]\n",
    "        \n",
    "        [o, s, s2], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            outputs_info=[None, \n",
    "                          dict(initial=T.zeros(self.hidden_dim)),\n",
    "                          dict(initial=T.zeros(self.hidden_dim))])\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Total cost (could add regularization here)\n",
    "        cost = o_error\n",
    "        \n",
    "        # Gradients\n",
    "        dE = T.grad(cost, E)\n",
    "        dU = T.grad(cost, U)\n",
    "        dW = T.grad(cost, W)\n",
    "        db = T.grad(cost, b)\n",
    "        dV = T.grad(cost, V)\n",
    "        dc = T.grad(cost, c)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.predict = theano.function([x], o)\n",
    "        self.predict_class = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], cost)\n",
    "        self.bptt = theano.function([x, y], [dE, dU, dW, db, dV, dc])\n",
    "        \n",
    "        # SGD parameters\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        \n",
    "        # rmsprop cache updates\n",
    "        mE = decay * self.mE + (1 - decay) * T.sqr(dE)\n",
    "        mU = decay * self.mU + (1 - decay) * T.sqr(dU)\n",
    "        mW = decay * self.mW + (1 - decay) * T.sqr(dW)\n",
    "        mV = decay * self.mV + (1 - decay) * T.sqr(dV)\n",
    "        mb = decay * self.mb + (1 - decay) * T.sqr(db)\n",
    "        mc = decay * self.mc + (1 - decay) * T.sqr(dc)\n",
    "        \n",
    "        self.sgd_step = theano.function(\n",
    "            [x, y, learning_rate, theano.Param(decay, default=0.9)],\n",
    "            [], \n",
    "            updates=[(E, E - learning_rate * dE / T.sqrt(mE + 1e-6)),\n",
    "                     (U, U - learning_rate * dU / T.sqrt(mU + 1e-6)),\n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-6)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-6)),\n",
    "                     (b, b - learning_rate * db / T.sqrt(mb + 1e-6)),\n",
    "                     (c, c - learning_rate * dc / T.sqrt(mc + 1e-6)),\n",
    "                     (self.mE, mE),\n",
    "                     (self.mU, mU),\n",
    "                     (self.mW, mW),\n",
    "                     (self.mV, mV),\n",
    "                     (self.mb, mb),\n",
    "                     (self.mc, mc)\n",
    "                    ])\n",
    "        \n",
    "        \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load parameters of pre-trained model\n",
    "# model = load_model_parameters_theano('./data/2015-10-18/GRUTheano-80-8000-10.npz', GRUTheano)\n",
    "# model = load_model_parameters_theano('./data/pretrained.npz', GRUTheano)\n",
    "\n",
    "# Build model and train\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GRUTheano(VOCABULARY_SIZE, hidden_dim=HIDDEN_DIM, bptt_truncate=-1)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[10], y_train[10], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train[:500], y_train[:500], LEARNING_RATE, NEPOCH, evaluate_loss_after=1, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do . managed gt ; on was UNKNOWN_TOKEN that n't more park it you so too from so , and attempt ) , 's who you ... friends has it from type .\n",
      "UNKNOWN_TOKEN keep right , guy that , & has she UNKNOWN_TOKEN to final .\n",
      "i holding that request and any 's be .. the ready the i degree their make 25 his UNKNOWN_TOKEN the at for eat a over of UNKNOWN_TOKEN it .\n",
      "she world a too account , i please : from still UNKNOWN_TOKEN lost `` various .\n",
      "this building 's the '' of UNKNOWN_TOKEN .\n",
      "gt just them is as the ^ done UNKNOWN_TOKEN .\n",
      "if i & be can UNKNOWN_TOKEN upset show and been be and and UNKNOWN_TOKEN but you terms UNKNOWN_TOKEN to entire be show .\n",
      "'ve were along religion , it teach be these .\n",
      "UNKNOWN_TOKEN and they a trade career .\n",
      "ways and have UNKNOWN_TOKEN would ) is my a really has year and i under\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, index_to_word, word_to_index):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)[-1]\n",
    "        samples = np.random.multinomial(1, next_word_probs)\n",
    "        sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        # Seomtimes we get stuck in an infinite loop if the sentence becomes too long (e.g. .....) :(\n",
    "        if len(new_sentence) > 100:\n",
    "          return []\n",
    "    return new_sentence\n",
    "\n",
    "def print_sentence(s, index_to_word):\n",
    "  sentence_str = [index_to_word[x] for x in s[1:-1]]\n",
    "  print(\" \".join(sentence_str))\n",
    "  sys.stdout.flush()\n",
    "\n",
    "def generate_sentences(model, n, index_to_word, word_to_index, min_length=5):\n",
    "  for i in range(n):\n",
    "      sent = []\n",
    "      while len(sent) < min_length:\n",
    "          sent = generate_sentence(model, index_to_word, word_to_index)\n",
    "      print_sentence(sent, index_to_word)\n",
    "\n",
    "generate_sentences(model, 10, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
