{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from theano.gradient import grad_clip\n",
    "from utils import *\n",
    "from lstm_theano import *\n",
    "from gru_theano import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 396962 sentences.\n",
      "Found 224364 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'pig' and appeared 53 times.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from utils import load_and_proprocess_data\n",
    "VOCABULARY_SIZE = 8000\n",
    "X_train, y_train, word_to_index, index_to_word = load_and_proprocess_data(VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5693"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (3, hidden_dim, word_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (3, hidden_dim, hidden_dim))\n",
    "        b = np.zeros((3, hidden_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        b2 = np.zeros(word_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        # Bias terms\n",
    "        self.b = theano.shared(name='b_i', value=b.astype(theano.config.floatX))\n",
    "        self.b2 = theano.shared(name='b_V', value=b2.astype(theano.config.floatX))\n",
    "        # SGD: Initialize parameters\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        self.mb = theano.shared(name='mb', value=np.zeros(b.shape).astype(theano.config.floatX))\n",
    "        self.mb2 = theano.shared(name='mb2', value=np.zeros(b2.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        V, U, W, b, b2 = self.V, self.U, self.W, self.b, self.b2\n",
    "        mV, mU, mW, mb, mb2 = self.mV, self.mU, self.mW, self.mb, self.mb2\n",
    "        \n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t_prev):\n",
    "            # This is how we calculated the hidden state in a simple RNN. No longer!\n",
    "            # s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            \n",
    "            # Clip the gradients\n",
    "            W_clipped = grad_clip(W, -1, 1)\n",
    "            U_clipped = grad_clip(U, -1, 1)\n",
    "            V_clipped = grad_clip(V, -1, 1)\n",
    "            b_clipped = grad_clip(b, -1, 1)\n",
    "            b2_clipped = grad_clip(b2, -1, 1)\n",
    "            \n",
    "            # LRU hidden state calculation\n",
    "            z_t = T.nnet.sigmoid(U_clipped[0][:,x_t] + W_clipped[0].dot(s_t_prev) + b_clipped[0])\n",
    "            r_t = T.nnet.sigmoid(U_clipped[1][:,x_t] + W_clipped[1].dot(s_t_prev) + b_clipped[1])\n",
    "            c_t = T.tanh(U_clipped[2][:,x_t] + W_clipped[2].dot(s_t_prev) * r_t + b_clipped[2])\n",
    "            s_t = (1 - z_t) * c_t + z_t * s_t_prev\n",
    "              \n",
    "            # Final output calculation\n",
    "            # Theano's softmax returns a matrix with one row, we only need the row\n",
    "            o_t = T.nnet.softmax(V_clipped.dot(s_t) + b2_clipped)[0]\n",
    "\n",
    "            return [o_t, s_t]\n",
    "        \n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            truncate_gradient=self.bptt_truncate)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dW = T.grad(o_error, W)\n",
    "        db = T.grad(o_error, b)\n",
    "        dV = T.grad(o_error, V)\n",
    "        db2 = T.grad(o_error, b2)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dW, db, dV, db2])\n",
    "        \n",
    "        # SGD with Momentum\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        self.sgd_step = theano.function(\n",
    "            [x, y, learning_rate, theano.Param(decay, default=0.99)],\n",
    "            [], \n",
    "            updates=[(U, U - learning_rate * dU / T.sqrt(mU + 1e-8)),                     \n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-8)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-8)),\n",
    "                     (b, b - learning_rate * db / T.sqrt(mb + 1e-8)),\n",
    "                     (b2, b2 - learning_rate * db2 / T.sqrt(mb2 + 1e-8)),\n",
    "                     (mU, decay * mU + (1 - decay) * T.sqr(dU)),\n",
    "                     (mW, decay * mW + (1 - decay) * T.sqr(dW)),\n",
    "                     (mV, decay * mV + (1 - decay) * T.sqr(dV)),\n",
    "                     (mb, decay * mb + (1 - decay) * T.sqr(db)),\n",
    "                     (mb2, decay * mb2 + (1 - decay) * T.sqr(db2))\n",
    "                    ])\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 3000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter W with size 300.\n",
      "Gradient check for parameter W passed.\n",
      "Performing gradient check for parameter b with size 30.\n",
      "Gradient check for parameter b passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter b2 with size 100.\n",
      "Gradient check for parameter b2 passed.\n"
     ]
    }
   ],
   "source": [
    "# Do a gradient check\n",
    "np.random.seed(0)\n",
    "model = GRUTheano(100, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model model from ./data/pretrained.npz with hidden_dim=80 word_dim=8000\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of pre-trained model\n",
    "model = load_model_parameters_theano('./data/pretrained.npz', GRUTheano)\n",
    "\n",
    "# Build model and train\n",
    "\n",
    "# LEARNING_RATE = 1e-5\n",
    "# NEPOCH = 50\n",
    "# HIDDEN_DIM = 80\n",
    "\n",
    "# model = GRUTheano(VOCABULARY_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "# t1 = time.time()\n",
    "# model.sgd_step(X_train[10], y_train[10], LEARNING_RATE)\n",
    "# t2 = time.time()\n",
    "# print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "# train_with_sgd(model, X_train[:500], y_train[:500], LEARNING_RATE, NEPOCH, evaluate_loss_after=1)\n",
    "# save_model_parameters_theano('data/mymodel.npz', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 http : http : http : i do .\n",
      "0.000000 http : of the same .\n",
      "0.000000 http : http : is a them the .\n",
      "0.000000 http : a pretty feel ?\n",
      "0.000000 https : https : out .\n",
      "0.000000 http : me of a !\n",
      "0.000000 http : //www.youtube.com/watch we have .\n",
      "0.000363 http : http : //www.youtube.com/watch .\n",
      "0.000000 http : http : http : as well .\n",
      "0.000000 http : least the them .\n",
      "0.000179 http : http : http : )\n",
      "0.000000 http : http : what it .\n",
      "0.000025 http : https : //www.youtube.com/watch ?\n",
      "0.000057 http : http : ( )\n",
      "0.000041 http : http : ( i.e .\n",
      "0.000000 http : http : http : a one .\n",
      "0.000000 http : against the while .\n",
      "0.000000 http : the do about .\n",
      "0.000000 http : your good either .\n",
      "0.000076 http : http : http : p\n",
      "0.000088 http : http : all .\n",
      "0.000000 http : , but your months .\n",
      "0.000000 http : the one 's to the of it .\n",
      "0.000000 http : to the their own year .\n",
      "0.000000 http : you was a time .\n",
      "0.000179 http : http : http : )\n",
      "0.000000 http : //www.youtube.com/watch he was .\n",
      "0.000164 http : http : it .\n",
      "0.000000 http : http : http : the a are not .\n",
      "0.000001 http : http : all this .\n",
      "0.000164 http : http : it .\n",
      "0.000080 http : http : ) )\n",
      "0.000000 http : http : best game .\n",
      "0.000000 http : http : http : all a the down .\n",
      "0.000000 http : //www.youtube.com/watch they 're i .\n",
      "0.000021 http : http : http : ''\n",
      "0.000000 http : to be talking .\n",
      "0.000000 http : //www.youtube.com/watch to the bad .\n",
      "0.000000 https : http : , you know .\n",
      "0.000000 https : the what 's try .\n",
      "0.000000 http : , i get .\n",
      "0.000005 https : http : the .\n",
      "0.000015 http : http : you .\n",
      "0.000057 http : http : ( )\n",
      "0.000001 http : http : it out .\n",
      "0.000196 http : http : http : .\n",
      "0.000000 http : http : it as well .\n",
      "0.000196 http : http : http : .\n",
      "0.000000 http : http : or ) .\n",
      "0.000179 http : http : http : )\n"
     ]
    }
   ],
   "source": [
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    \n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token and a random word\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    prob = 1\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        samples = np.random.multinomial(5, next_word_probs[-1])\n",
    "        sampled_word = np.argmax(samples)\n",
    "        prob = prob * next_word_probs[-1][sampled_word]\n",
    "        # Discard this sentence if we sample an unknown word\n",
    "        if sampled_word == word_to_index[unknown_token]:\n",
    "           return [1,[]]\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return [prob, sentence_str]\n",
    " \n",
    "num_sentences = 50\n",
    "senten_min_length = 6\n",
    "sentences = []\n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        prob, sent = generate_sentence(model)\n",
    "    print \"%f %s\" % (prob, \" \".join(sent))\n",
    "    sentences.append([prob, sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
