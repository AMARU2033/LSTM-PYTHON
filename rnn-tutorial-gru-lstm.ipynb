{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "from theano.gradient import grad_clip\n",
    "from utils import *\n",
    "from lstm_theano import *\n",
    "from gru_theano import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 65829 sentences.\n",
      "Found 57068 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'midnight' and appeared 9 times.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from utils import load_and_proprocess_data\n",
    "VOCABULARY_SIZE = 8000\n",
    "X_train, y_train, word_to_index, index_to_word = load_and_proprocess_data(VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a gradient check\n",
    "np.random.seed(0)\n",
    "model = GRUTheano(100, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (3, hidden_dim, word_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (3, hidden_dim, hidden_dim))\n",
    "        b = np.zeros((3, hidden_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        b2 = np.zeros(word_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        # Bias terms\n",
    "        self.b = theano.shared(name='b_i', value=b.astype(theano.config.floatX))\n",
    "        self.b2 = theano.shared(name='b_V', value=b2.astype(theano.config.floatX))\n",
    "        # SGD: Initialize parameters\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        self.mb = theano.shared(name='mb', value=np.zeros(b.shape).astype(theano.config.floatX))\n",
    "        self.mb2 = theano.shared(name='mb2', value=np.zeros(b2.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        V, U, W, b, b2 = self.V, self.U, self.W, self.b, self.b2\n",
    "        # mV, mU, mW, mb, mb2 = self.mV, self.mU, self.mW, self.mb, self.mb2\n",
    "        \n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t_prev):\n",
    "            # This is how we calculated the hidden state in a simple RNN. No longer!\n",
    "            # s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            \n",
    "            # Clip the gradients\n",
    "            W_clipped = grad_clip(W, -1, 1)\n",
    "            U_clipped = grad_clip(U, -1, 1)\n",
    "            V_clipped = grad_clip(V, -1, 1)\n",
    "            b_clipped = grad_clip(b, -1, 1)\n",
    "            b2_clipped = grad_clip(b2, -1, 1)\n",
    "            \n",
    "            # LRU hidden state calculation\n",
    "            z_t = T.nnet.sigmoid(U_clipped[0][:,x_t] + W_clipped[0].dot(s_t_prev) + b_clipped[0])\n",
    "            r_t = T.nnet.sigmoid(U_clipped[1][:,x_t] + W_clipped[1].dot(s_t_prev) + b_clipped[1])\n",
    "            c_t = T.tanh(U_clipped[2][:,x_t] + W_clipped[2].dot(s_t_prev) * r_t + b_clipped[2])\n",
    "            s_t = (1 - z_t) * c_t + z_t * s_t_prev\n",
    "              \n",
    "            # Final output calculation\n",
    "            # Theano's softmax returns a matrix with one row, we only need the row\n",
    "            o_t = T.nnet.softmax(V_clipped.dot(s_t) + b2_clipped)[0]\n",
    "\n",
    "            return [o_t, s_t]\n",
    "        \n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            truncate_gradient=self.bptt_truncate)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dW = T.grad(o_error, W)\n",
    "        db = T.grad(o_error, b)\n",
    "        dV = T.grad(o_error, V)\n",
    "        db2 = T.grad(o_error, b2)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dW, db, dV, db2])\n",
    "        \n",
    "        # SGD parameters\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        \n",
    "        # rmsprop cache updates\n",
    "        mU = decay * self.mU + (1 - decay) * T.sqr(dU)\n",
    "        mW = decay * self.mW + (1 - decay) * T.sqr(dW)\n",
    "        mV = decay * self.mV + (1 - decay) * T.sqr(dV)\n",
    "        mb = decay * self.mb + (1 - decay) * T.sqr(db)\n",
    "        mb2 = decay * self.mb2 + (1 - decay) * T.sqr(db2)\n",
    "        \n",
    "        self.sgd_step = theano.function(\n",
    "            [x, y, learning_rate, theano.Param(decay, default=0.9)],\n",
    "            [], \n",
    "            updates=[(U, U - learning_rate * dU / T.sqrt(mU + 1e-8)),                     \n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-8)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-8)),\n",
    "                     (b, b - learning_rate * db / T.sqrt(mb + 1e-8)),\n",
    "                     (b2, b2 - learning_rate * db2 / T.sqrt(mb2 + 1e-8)),\n",
    "                     (self.mU, mU),\n",
    "                     (self.mW, mW),\n",
    "                     (self.mV, mV),\n",
    "                     (self.mb, mb),\n",
    "                     (self.mb2, mb2)\n",
    "                    ])\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: 289.343119 milliseconds\n",
      "2015-10-16 23:32:05: Loss after num_examples_seen=0 epoch=0: 8.984983\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of pre-trained model\n",
    "# model = load_model_parameters_theano('./data/pretrained.npz', GRUTheano)\n",
    "\n",
    "# Build model and train\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 50\n",
    "HIDDEN_DIM = 80\n",
    "\n",
    "model = GRUTheano(VOCABULARY_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[10], y_train[10], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train[:500], y_train[:500], LEARNING_RATE, NEPOCH, evaluate_loss_after=1)\n",
    "# save_model_parameters_theano('data/mymodel.npz', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 tips solve death happiness dispute it .\n",
      "0.000000 elsewhere riven statement dynamic imo ?\n",
      "0.000000 score consoles the soul % the so .\n",
      "0.000000 operating symbol retarded item fandom it it .\n",
      "0.000000 considerably fiction disease background was include that .\n",
      "0.000000 q 140 loss agree for chapter .\n",
      "0.000000 eaten framework too wage long .\n",
      "0.000000 trap hierarchy idea manager : mod ) `` .\n",
      "0.000000 well champs emotionally discussed . ''\n",
      "0.000000 intellectually led technically attack the demon attack knows .\n",
      "0.000000 100 windows boost pain with numbers .\n",
      "0.000000 albeit varied clay cpu large was the lol practice .\n",
      "0.000000 jeg bottle organic % . '' .\n",
      "0.000000 nations sends case detailed option teens ideas this safe .\n",
      "0.000000 weighs pe no just ) .\n",
      "0.000000 overkill any use election jumper .\n",
      "0.000000 tagged photoshop properties other disabled it .\n",
      "0.000000 unrealistic comparable performance just genius .\n",
      "0.000000 style seven canada faster voluntary .\n",
      "0.000000 subreddit iq above happiness against .\n",
      "0.000000 rock understand brand like the for now 30\n",
      "0.000000 cruise что rewrite yield relationships .\n",
      "0.000000 batman asylum half blood coins discussed yourselves ?\n",
      "0.000000 bikes charges reaction my n't .\n",
      "0.000000 lifts prince shows disagreeing you the battery .\n",
      "0.000000 80 truth related silk first ! .\n",
      "0.000000 abilities album battlefield read adjusted '' .\n",
      "0.000000 expert mapping nicotine control multiple .\n",
      "0.000000 360 tears portrayed confused ) into about do .\n",
      "0.000000 zombie sound bundle people midrange scary your and murder imo and genius .\n",
      "0.000000 12. grain hardest plot about .\n",
      "0.000000 collecting supposed refuses link through .\n",
      "0.000000 den 10-15 om product been the .\n",
      "0.000000 will language sandwich free '' .\n",
      "0.000000 sigelei makeup k interact ships bad .\n",
      "0.000000 lid servers late floor with it .\n",
      "0.000000 pet arrest detailed badly gear .\n",
      "0.000000 beaten payments 7. into '' .\n",
      "0.000000 flames your faith guide today .\n",
      "0.000000 plug vayne 5000 ranked immigration it .\n",
      "0.000000 normal creates grad everyone you our .\n",
      "0.000000 trauma angel for it the playthrough .\n",
      "0.000000 war introduced olds one the ass .\n",
      "0.000000 slower anymore sprint unlock your '' top .\n",
      "0.000000 could interviews similar point as .\n",
      "0.000000 ruin blake lifestyle driving onto .\n",
      "0.000000 port somewhere dating used say upon scam of pain years was of too .\n",
      "0.000000 correctness forgetting yelling farming for veggies use you .\n",
      "0.000000 pursuit red mass account was .\n",
      "0.000000 argued arizona was subject do to it .\n"
     ]
    }
   ],
   "source": [
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    \n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token and a random word\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    prob = 1\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "        sampled_word = np.argmax(samples)\n",
    "        prob = prob * next_word_probs[-1][sampled_word]\n",
    "        # Discard this sentence if we sample an unknown word\n",
    "        if sampled_word == word_to_index[unknown_token]:\n",
    "           return [1,[]]\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return [prob, sentence_str]\n",
    " \n",
    "num_sentences = 50\n",
    "senten_min_length = 6\n",
    "sentences = []\n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        prob, sent = generate_sentence(model)\n",
    "    print \"%f %s\" % (prob, \" \".join(sent))\n",
    "    sentences.append([prob, sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
